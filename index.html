<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="figs/bu_logo.png">
  <title>Rabindra Nath Nandi</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>

      <!-- ABOUT -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="75%" valign="middle">
        <p align="center">
          <name>Rabindra Nath Nandi</name>
        </p>
        <p style="text-align:justify"><br>

          I have 4.5 year working experiences in software industries. Currently I am working in <a href="https://www.bjitgroup.com//">BJIT</a>  with the development of Artificial Intelligence based solution for Video Surveillance System, Road Security, Intelligent Chat-bot and Real-time analytic platform.

          I am a first year PhD candidate at <a href="http://www.bu.edu/">Boston University</a> in the <a href="https://www.bu.edu/cs/ivc/">Image &amp; Video Computing</a> group advised by <a href="http://www.cs.bu.edu/fac/sclaroff/">Prof. Stan Sclaroff</a>. I am interested in computer vision, machine learning, statistics and representation learning.
        </p>

        <p style="text-align:justify">
          Previously, I obtained my B.Sc. in Computer Science and Engineering from <a href="http://www.kuet.ac.bd/">KUET</a>. I also worked as an intern at <a href="https://www.csail.mit.edu/">MIT CSAIL</a> with <a href="https://kalyan.lids.mit.edu/">Dr. Kalyan Veeramachaneni</a> and <a href="https://people.csail.mit.edu/lkagal/">Dr. Lalana Kagal</a>.
        </p>

        <p style="text-align:justify">
          
        </p>
        <p align=center>
          rabindra.nath [at] bjitgroup.com &nbsp|&nbsp
          <a href="docs/nataniel_cv_long.pdf">CV</a> &nbsp|&nbsp
          <a href="https://scholar.google.fr/citations?user=MCHXKrUAAAAJ=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/robi56">GitHub</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/rnandi/"> LinkedIn </a>
        </p>
        </td>
        <td width="25%">
        <img src="figs/headshot_circle.png">
        </td>
      </tr>
      </table>

      <!-- RESEARCH -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
            I explored several topics in computer vision including scene understanding, object detection, semantic segmentation, vehicle re-identification and tracking for video survillenge application.
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr>
      <td width="25%">
        <div class="one">
        <img src='figs/lts_sim.png'>
        </div>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://www.researchgate.net/profile/M_A_H_Akhand/publication/280154794_Gene_Regulatory_Network_Inference_incorporating_Maximal_Information_Coefficient_into_Minimal_Redundancy_Network/links/575262df08ae10d93370ea0a/Gene-Regulatory-Network-Inference-incorporating-Maximal-Information-Coefficient-into-Minimal-Redundancy-Network.pdf">
          <papertitle>Gene Regulatory Network Inference Using Maximal Information Coefficient</papertitle></a><br>
          <strong>MAH Akhand, RN Nandi, SM Amran, K Murase</strong>
          <em>International Conference on Learning Representations (ICLR)</em>, 2019 <br>
          <p style="text-align:justify">
           MIC incorporation using two existing poluar methods and get better  result
          </p>
        </td>
      </tr>

    <tr>
      <td width="25%">
        <div class="one">
        <img src='figs/gaze.png'>
        </div>
      </td>
      <td valign="top" width="75%">
        <p><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper.html">
          <papertitle>Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency</papertitle></a><br>
          <a href="https://www.cc.gatech.edu/~echong8/">E. Chong</a>, <strong>N. Ruiz</strong>, Y. Wang, <a href="https://sites.google.com/view/yunzhang/home">Y. Zhang</a>, <a href="http://www.agatarozga.org/">A. Rozga</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
          <em>European Conference on Computer Vision (ECCV)</em>, 2018 <br>
          <a href="docs/argos_poster.pdf">poster</a> &nbsp/&nbsp <a href="docs/Chong_2018_ECCV.bib">bibtex</a><br>
          <p style="text-align:justify">
            We are the first to tackle the generalized visual attention prediction problem, which consists in predicting the 3D gaze vector, attention heatmaps inside of the image frame and whether the subject is looking inside or outside of the image. To this end, we jointly model gaze and scene saliency using a neural network architecture trained on three heterogeneous datasets.
          </p>
        </td>
      </tr>

      <tr>
        <td width="25%">
          <div class="one">
          <img src='figs/deep-head-pose.png'>
          </div>
        </td>
        <td valign="top" width="75%">
          <p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w41/html/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.html">
            <papertitle>Fine-Grained Head Pose Estimation Without Keypoints</papertitle></a><br>
            <strong>N. Ruiz</strong>, <a href="https://www.cc.gatech.edu/~echong8/">E. Chong</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
            <em>Computer Vision and Pattern Recognition Workshop (CVPRW)</em>, 2018 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font><br>
            <a href="https://github.com/natanielruiz/deep-head-pose">code</a> &nbsp/&nbsp <a href="https://youtu.be/Bz6eF4Nl1O8">video demo</a> &nbsp/&nbsp <a href="docs/hopenet_poster.pdf">poster</a> &nbsp/&nbsp <a href="docs/ruiz2017fine.bib">bibtex</a><br>
            <p style="text-align:justify">
              By using a deep network trained with a binned pose classification loss and a pose regression loss on a large dataset we obtain state-of-the-art head pose estimation results on several popular benchmarks. Our head pose estimation models generalize to different domains and work on low-resolution images. We release an open-source software package with pre-trained models which can be used directly on images and video.
            </p>
          </td>
        </tr>

      <tr>
        <td width="25%">
          <div class="one">
          <img src='figs/localize-and-align.png'>
          </div>
        </td>
        <td valign="top" width="75%">
          <p><a href="https://arxiv.org/abs/1809.08381">
            <papertitle>Learning to Localize and Align Fine­-Grained Actions to Sparse Instructions</papertitle></a><br>
            <a href="http://www.meerahahn.net/">M. Hahn</a>, <strong>N. Ruiz</strong>, <a href="http://www.di.ens.fr/~alayrac/">J.B. Alayrac</a>, <a href="http://www.di.ens.fr/~laptev/">I. Laptev</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
            <em>arXiv Preprint</em>, 2018 <br>
            <p style="text-align:justify">
              We present a framework which, given an instructional video, can localize atomic action segments and align them to the appropriate instructional step using object recognition and natural language.
            </p>
        </td>
      </tr>

      <tr>
        <td width="25%">
          <div class="one">
          <img src='figs/eye-contact.png'>
          </div>
        </td>
        <td valign="top" width="75%">
          <p><a href="https://dl.acm.org/citation.cfm?id=3131902">
            <papertitle>Detecting Gaze Towards Eyes in Natural Social Interactions and Its Use in Child Assessment</papertitle></a><br>
            <a href="https://www.cc.gatech.edu/~echong8/">E. Chong</a>, <a href="https://www.linkedin.com/in/kathachanda/">K. Chanda</a>, <a href="https://scholar.google.com/citations?user=EKAynJgAAAAJ&hl=en">Z. Ye</a>, <a href="https://www.cc.gatech.edu/people/audrey-southerland">A. Southerland</a>, <strong>N. Ruiz</strong>, <a href="http://vivo.med.cornell.edu/display/cwid-rej2004">R.M. Jones</a>, <a href="http://www.agatarozga.org/">A. Rozga</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
            <em>UbiComp and Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</em>, 2017<br> 
            <font color="red"><strong>(Oral Presentation and Distinguished Paper Award - 3% award rate)</strong></font><br>
            <a href="docs/chong2017imwut.bib">bibtex</a><br>
            <p style="text-align:justify">
              We introduce the Pose-Implicit CNN, a novel deep learning architecture that predicts eye contact while implicitly estimating the head pose. The model is trained on a dataset comprising 22 hours of 156 play session videos from over 100 children, half of whom are diagnosed with Autism Spectrum Disorder.
            </p>
        </td>
      </tr>

      <tr>
      <td width="25%">
        <div class="one">
        <img src='figs/dockerface.jpg'>
        </div>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/1708.04370">
          <papertitle>Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a Docker Container</papertitle></a><br>
          <strong>N. Ruiz</strong>, <a href="http://rehg.org/">J.M. Rehg</a><br>
          <em>arXiv Preprint</em>, 2017 <br>
          <a href="https://github.com/natanielruiz/dockerface">code</a> &nbsp/&nbsp <a href="docs/ruiz2017dockerface.bib">bibtex</a><br> 
          <p style="text-align:justify">
            In order to help the wider scientific community, we release a pre-trained deep learning face detector which is easy to download and use on images and video.
          </p>
        </td>
      </tr>

      </table>

      <!-- PROJECTS -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Projects</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">

      <table width="100%" align="center" border="0" cellpadding="20">
        <tr>
          <td width="25%"><img src="figs/yolo.png" width="160" height="160"></td>
          <td width="75%" valign="top">
          <p>
            <a href="https://github.com/natanielruiz/android-yolo">
            <papertitle>android-yolo</papertitle>
            </a>
            <br>
            <strong>N. Ruiz</strong>
            <br>
            <a href="https://youtu.be/EhMrf4G5Wf0">video demo</a> &nbsp/&nbsp <a href="https://drive.google.com/open?id=0B2fFW2t9-qW3LWFDNXVHUE9rV3M">app apk</a><br>
          <p style="text-align:justify">
            Real-time object detection on Android using the YOLO network with TensorFlow.
          </p>
          </p>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
          <br>
          <p align="right">
            <font size="1">
            <a href="https://jonbarron.info/">template</a>
        </font>
          </p>
          </td>
        </tr>
        </table>

      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </table>
  </body>
</html>
