<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="figs/bu_logo.png">
  <title>Nataniel Ruiz</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>

      <!-- ABOUT -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="75%" valign="middle">
        <p align="center">
          <name>Nataniel Ruiz</name>
        </p>
        <p style="text-align:justify"><br>
          I am a first year PhD candidate at <a href="http://www.bu.edu/">Boston University</a> in the <a href="https://www.bu.edu/cs/ivc/">Image &amp; Video Computing</a> group advised by <a href="http://www.cs.bu.edu/fac/sclaroff/">Prof. Stan Sclaroff</a>. I am interested in computer vision, machine learning, statistics and representation learning. 
        </p>

        <p style="text-align:justify">
          I will be interning at <a href="https://machinelearning.apple.com/">Apple AI Research</a> during Summer 2019. In 2018 I was a Spring/Summer intern at the <a href="http://www.nec-labs.com/research-departments/media-analytics/media-analytics-home">NEC-Labs Media Analytics Department</a>, where I worked with <a href="http://www.nec-labs.com/~manu/">Prof. Manmohan Chandraker</a> and <a href="http://www.nec-labs.com/samuel-schulter">Dr. Samuel Schulter</a>. I graduated from <a href="http://www.gatech.edu/">Georgia Tech</a> in Fall 2017 with a M.Sc. in Computer Science specializing in Machine Learning, advised by <a href="http://rehg.org/">Prof. James Rehg</a> at the <a href="http://behavioralimaging.net/index.php">Center for Behavioral Imaging</a>.<br><br>
          
          Previously, I obtained my B.Sc. and M.Sc. in Data Science from <a href="https://www.polytechnique.edu/en">Ecole Polytechnique</a>. I also worked as an intern at <a href="https://www.csail.mit.edu/">MIT CSAIL</a> with <a href="https://kalyan.lids.mit.edu/">Dr. Kalyan Veeramachaneni</a> and <a href="https://people.csail.mit.edu/lkagal/">Dr. Lalana Kagal</a>.
        </p>

        <p style="text-align:justify">
          
        </p>
        <p align=center>
          nruiz9 [at] bu.edu &nbsp|&nbsp
          <a href="docs/nataniel_cv_long.pdf">CV</a> &nbsp|&nbsp
          <a href="https://scholar.google.fr/citations?user=CiOmcSIAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://github.com/natanielruiz">GitHub</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/nataniel-ruiz/"> LinkedIn </a>
        </p>
        </td>
        <td width="25%">
        <img src="figs/headshot_circle.png">
        </td>
      </tr>
      </table>

      <!-- RESEARCH -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Research</heading>
          <p>
            I explored several topics in computer vision including face and gesture analysis, scene understanding, first person vision, instructional video understanding and mobile computer vision. During my internship at NEC Labs I worked on topics related to self-driving car perception, visual data simulation and reinforcement learning.
          </p>
        </td>
      </tr>
      </table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr>
      <td width="25%">
        <div class="one">
        <img src='figs/lts_sim.png'>
        </div>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://openreview.net/forum?id=HJgkx2Aqt7&noteId=HJgkx2Aqt7">
          <papertitle>Learning To Simulate</papertitle></a><br>
          <strong>N. Ruiz</strong>, <a href="http://www.nec-labs.com/samuel-schulter">S. Schulter</a>, <a href="http://www.nec-labs.com/~manu/">M. Chandraker</a><br>
          <em>International Conference on Learning Representations (ICLR)</em>, 2019 <br>
          <a href="docs/lts_poster.pdf">poster</a><br>
          <p style="text-align:justify">
            We propose an algorithm that automatically learns parameters of a simulation engine to generate training data for a machine learning model in order to maximize performance. We present experiments on a toy example, an object counting vision task and on semantic segmentation for traffic scenes both on simulated and real evaluation data.
          </p>
        </td>
      </tr>

    <tr>
      <td width="25%">
        <div class="one">
        <img src='figs/gaze.png'>
        </div>
      </td>
      <td valign="top" width="75%">
        <p><a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper.html">
          <papertitle>Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency</papertitle></a><br>
          <a href="https://www.cc.gatech.edu/~echong8/">E. Chong</a>, <strong>N. Ruiz</strong>, Y. Wang, <a href="https://sites.google.com/view/yunzhang/home">Y. Zhang</a>, <a href="http://www.agatarozga.org/">A. Rozga</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
          <em>European Conference on Computer Vision (ECCV)</em>, 2018 <br>
          <a href="docs/argos_poster.pdf">poster</a> &nbsp/&nbsp <a href="docs/Chong_2018_ECCV.bib">bibtex</a><br>
          <p style="text-align:justify">
            We are the first to tackle the generalized visual attention prediction problem, which consists in predicting the 3D gaze vector, attention heatmaps inside of the image frame and whether the subject is looking inside or outside of the image. To this end, we jointly model gaze and scene saliency using a neural network architecture trained on three heterogeneous datasets.
          </p>
        </td>
      </tr>

      <tr>
        <td width="25%">
          <div class="one">
          <img src='figs/deep-head-pose.png'>
          </div>
        </td>
        <td valign="top" width="75%">
          <p><a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w41/html/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.html">
            <papertitle>Fine-Grained Head Pose Estimation Without Keypoints</papertitle></a><br>
            <strong>N. Ruiz</strong>, <a href="https://www.cc.gatech.edu/~echong8/">E. Chong</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
            <em>Computer Vision and Pattern Recognition Workshop (CVPRW)</em>, 2018 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font><br>
            <a href="https://github.com/natanielruiz/deep-head-pose">code</a> &nbsp/&nbsp <a href="https://youtu.be/Bz6eF4Nl1O8">video demo</a> &nbsp/&nbsp <a href="docs/hopenet_poster.pdf">poster</a> &nbsp/&nbsp <a href="docs/ruiz2017fine.bib">bibtex</a><br>
            <p style="text-align:justify">
              By using a deep network trained with a binned pose classification loss and a pose regression loss on a large dataset we obtain state-of-the-art head pose estimation results on several popular benchmarks. Our head pose estimation models generalize to different domains and work on low-resolution images. We release an open-source software package with pre-trained models which can be used directly on images and video.
            </p>
          </td>
        </tr>

      <tr>
        <td width="25%">
          <div class="one">
          <img src='figs/localize-and-align.png'>
          </div>
        </td>
        <td valign="top" width="75%">
          <p><a href="https://arxiv.org/abs/1809.08381">
            <papertitle>Learning to Localize and Align Fine­-Grained Actions to Sparse Instructions</papertitle></a><br>
            <a href="http://www.meerahahn.net/">M. Hahn</a>, <strong>N. Ruiz</strong>, <a href="http://www.di.ens.fr/~alayrac/">J.B. Alayrac</a>, <a href="http://www.di.ens.fr/~laptev/">I. Laptev</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
            <em>arXiv Preprint</em>, 2018 <br>
            <p style="text-align:justify">
              We present a framework which, given an instructional video, can localize atomic action segments and align them to the appropriate instructional step using object recognition and natural language.
            </p>
        </td>
      </tr>

      <tr>
        <td width="25%">
          <div class="one">
          <img src='figs/eye-contact.png'>
          </div>
        </td>
        <td valign="top" width="75%">
          <p><a href="https://dl.acm.org/citation.cfm?id=3131902">
            <papertitle>Detecting Gaze Towards Eyes in Natural Social Interactions and Its Use in Child Assessment</papertitle></a><br>
            <a href="https://www.cc.gatech.edu/~echong8/">E. Chong</a>, <a href="https://www.linkedin.com/in/kathachanda/">K. Chanda</a>, <a href="https://scholar.google.com/citations?user=EKAynJgAAAAJ&hl=en">Z. Ye</a>, <a href="https://www.cc.gatech.edu/people/audrey-southerland">A. Southerland</a>, <strong>N. Ruiz</strong>, <a href="http://vivo.med.cornell.edu/display/cwid-rej2004">R.M. Jones</a>, <a href="http://www.agatarozga.org/">A. Rozga</a>, <a href="http://rehg.org/">J.M. Rehg</a><br>
            <em>UbiComp and Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT)</em>, 2017<br> 
            <font color="red"><strong>(Oral Presentation and Distinguished Paper Award - 3% award rate)</strong></font><br>
            <a href="docs/chong2017imwut.bib">bibtex</a><br>
            <p style="text-align:justify">
              We introduce the Pose-Implicit CNN, a novel deep learning architecture that predicts eye contact while implicitly estimating the head pose. The model is trained on a dataset comprising 22 hours of 156 play session videos from over 100 children, half of whom are diagnosed with Autism Spectrum Disorder.
            </p>
        </td>
      </tr>

      <tr>
      <td width="25%">
        <div class="one">
        <img src='figs/dockerface.jpg'>
        </div>
      </td>
      <td valign="top" width="75%">
        <p><a href="https://arxiv.org/abs/1708.04370">
          <papertitle>Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a Docker Container</papertitle></a><br>
          <strong>N. Ruiz</strong>, <a href="http://rehg.org/">J.M. Rehg</a><br>
          <em>arXiv Preprint</em>, 2017 <br>
          <a href="https://github.com/natanielruiz/dockerface">code</a> &nbsp/&nbsp <a href="docs/ruiz2017dockerface.bib">bibtex</a><br> 
          <p style="text-align:justify">
            In order to help the wider scientific community, we release a pre-trained deep learning face detector which is easy to download and use on images and video.
          </p>
        </td>
      </tr>

      </table>

      <!-- PROJECTS -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <heading>Projects</heading>
        </td>
      </tr>
      </table>
      <table width="100%" align="center" border="0" cellpadding="20">

      <table width="100%" align="center" border="0" cellpadding="20">
        <tr>
          <td width="25%"><img src="figs/yolo.png" width="160" height="160"></td>
          <td width="75%" valign="top">
          <p>
            <a href="https://github.com/natanielruiz/android-yolo">
            <papertitle>android-yolo</papertitle>
            </a>
            <br>
            <strong>N. Ruiz</strong>
            <br>
            <a href="https://youtu.be/EhMrf4G5Wf0">video demo</a> &nbsp/&nbsp <a href="https://drive.google.com/open?id=0B2fFW2t9-qW3LWFDNXVHUE9rV3M">app apk</a><br>
          <p style="text-align:justify">
            Real-time object detection on Android using the YOLO network with TensorFlow.
          </p>
          </p>
          </td>
        </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td>
          <br>
          <p align="right">
            <font size="1">
            <a href="https://jonbarron.info/">template</a>
        </font>
          </p>
          </td>
        </tr>
        </table>

      <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
          document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

      </script> <script type="text/javascript">
      try {
          var pageTracker = _gat._getTracker("UA-7580334-1");
          pageTracker._trackPageview();
          } catch(err) {}
      </script>
    </td>
    </tr>
  </table>
  </body>
</html>
